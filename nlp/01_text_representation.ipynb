{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tin_rAmAzJvv"
   },
   "source": [
    "# Text Repräsentationen\n",
    "In diesem Teil des Labors wird es um eine Einfürung in Text Repräsentationen gehen. \n",
    "\n",
    "Wie Ihr vermutlich schon wisst, repräsentieren Computer Daten in Binärcode. Um Euch das Problem, das Computer mit Texten haben aufzuzeigen, hat euch der Computer eine Nachricht hinterlassen, die ihr Entschlüsseln müsst... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Na68UqOJx8Jx"
   },
   "outputs": [],
   "source": [
    "binary_message = \"01001001 00100000 01110011 01110000 01100101 01100001 01101011 00100000 01100010 01101001 01101110 01100001 01110010 01111001 00100000 01101111 01101110 01101100 01111001 00100001 00001010 01001001 00100000 01101100 01101001 01101011 01100101 00100000 01101110 01110101 01101101 01100010 01100101 01110010 01110011 00101100 00100000 01101110 01101111 01110100 00100000 01110100 01100101 01111000 01110100 00101110\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "I_qao4JA1D0F"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I speak binary only!\n",
      "I like numbers, not text.\n"
     ]
    }
   ],
   "source": [
    "binary_chars = binary_message.split()\n",
    "message = \"\"\n",
    "for binary_char in binary_chars:\n",
    "    binary_char_integer = int(binary_char, 2)\n",
    "    character = chr(binary_char_integer)\n",
    "    message += character\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nMwRdW3J1WQv"
   },
   "source": [
    "Um Maschinen Text verständlich zu machen, müssen wir die Texte in eine andere Repräsentation bringen. Eine Kurze Einführung bieten die beiden folgenden Warm Up Aufgaben."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ioeYUuprcJOs"
   },
   "source": [
    "## Tokenization\n",
    "Eine der ersten elementaren Schritte, um eine natürliche Sprache mit einem Computer zu verarbeiten ist das Aufteilen eines Fließtextes in kleinere Teile. Meist werden Texte direkt in sogenannte Tokens geteilt (Tokenization). Ein Token ist oftmals ein einzelnes Wort, es kann aber auch aus mehreren Wörtern bestehen, wenn diese zusammen für eine Entität stehen, z.B. die Stadt „New York“. Neben der Aufteilung in Tokens ist es auch möglich, einen Text zuerst in Sätze zu splitten (Sentence Tokenization). Welche Kombination der Methoden angewendet wird, hängt von der Aufgabe ab. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6HFy1A2zjkqL"
   },
   "source": [
    "### Warm Up 1: Tokenizer mit Regulären Ausdrücken\n",
    "\n",
    "Die erste kleine Aufgabe besteht darin mit einem regulären Ausdruck ein Wort-Tokenizer zu bauen. (Diese Aufgabe sollte nicht mehr als 5 min. dauern) Der Tokenizer muss auch nicht perfekt sein. \n",
    "\n",
    "> Eine kurze Auffrischung über Reguläre Ausdrücke findet sich in https://regexr.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "vsACjYAEcHJc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Die', 'Ente', 'lacht', 'und', 'quakt', '']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "text=\"Die Ente lacht und quakt.\"\n",
    "\n",
    "tokenizer_regex=re.compile(r'[\\s.]')\n",
    "tokenizer_regex.split(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mzaasIkDmK_r"
   },
   "source": [
    "Das Grundprinzip eines Tokenizers sollte euch nun klar sein. In der Praxis empfiehlt es sich allerdings, auf bestehende Tokenizer-Implementierungen zurückzugreifen.\n",
    "Im Python-Umfeld ist https://spacy.io/api/tokenizer oder https://www.nltk.org/api/nltk.tokenize.html sehr empfehlenswert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hcOzQFueH9mC"
   },
   "source": [
    "# Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VqqIUO30B46n"
   },
   "source": [
    "Der nächste Schritt, um mit Texten Maschinelles Lernen zu betreiben, ist das Umwandeln der Wörter in eine numerissche Repräsentation. In der nächsten Warm-Up Aufgabe wird es darum um Bag of Words gehen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4-4dQzbG921p"
   },
   "source": [
    "## Warm Up 2: Bag of Words mit scikit-learn\n",
    "Ein Ansatz um Texte bzw. Tokens als Zahlen zu represäntieren ist ein Bag of Words. Hier wird jeder Text als Vektor dargestellt (Länge: Länge des Vokabulars). Jeder Eintrag im Vektor steht für ein Wort im Vokabular.\n",
    "\n",
    "In der nächsten Aufgabe wollen wir mit dem vorgegebenen Textkorpus, den Satz **\"Die Ente singt und quakt.\"** in einen bag-of-words-encoded Vector umwandeln.\n",
    "Dazu bietet sich der `CountVectorizer` an (siehe [Dokumentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)).\n",
    "\n",
    "> Kleiner Tipp: Der `CountVectorizer` übernimmt diesmal die Tokenisierung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ptA3RLihCfTj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['die', 'ente', 'lacht', 'quakt', 'singt', 'tanzt', 'und']\n",
      "[[1 1 1 1 0 0 1]\n",
      " [1 1 0 0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\"Die Ente lacht und quakt.\",\n",
    "          \"Die Ente singt und tanzt.\"]\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "abc =vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(abc.toarray())\n",
    "#bag_of_words_encoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_WWHn0N_4CT"
   },
   "source": [
    "Im  nächsten Schritt gebt die Länge der Satzrepräsentation aus. \n",
    "\n",
    "Wie verändert sich die Länge, wenn sich der Textkorpus vergrößert. Was sind die Vorteile und Nachteile von Bag of Words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "44zUnF0P_zDZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "size_of_sentence_encoded= abc.shape[1]\n",
    "print(size_of_sentence_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vorteil: Leicht zu verstehen \\\n",
    "Nachteil: Mit steigendem Vokabular steigt auch die Dimension des zugehörigen Vektors. Context und Semantik geht durch Sortierung verloren.\\\n",
    "( Es ist spät. vs. ist es spät?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dbCGNsjiYuAT"
   },
   "source": [
    "## Word Embeddings\n",
    "\n",
    "Natürliche Sprachen bieten uns mannigfaltige Möglichkeiten, dieselben Inhalte auf unterschiedliche Art und Weise auszudrücken. Wenn wir mit Texten arbeiten, reicht es daher in der Regel nicht aus, nur die Anzahl und das Vorkommen von bestimmten Wörtern in Texten zu betrachten, weil neben des bloßen Vorkommen eines Wortes auch dessen Anordnung und dessen Bedeutung eine Rolle spielt. Als zusätzliche und wichtige Ebene kommt hier also die Semantik ins Spiel.\n",
    "\n",
    "Die Abbildung von Wörtern auf Vektoren erlaubt es uns, mit diesen zu rechnen und zum Beispiel Distanzen oder Ähnlichkeiten zu bestimmen. Embeddings haben den Vorteil, dass sie eine Dimensionsreduktion mit sich bringen und semantische Embeddings sorgen darüber hinaus dafür, dass \"verwandte\" Wörter einen geringen Abstand voneinander haben.\n",
    "\n",
    "Wir wollen uns im Folgenden zunächst mit Word2Vec beschäftigen und eine einfache Version des CBOW-Ansatzes selbst implementieren.\n",
    "\n",
    "Danach schauen wir uns Gensim als Bibliothek für Word-Embedding-Modelle an und werfen einen Blick auf Evaluationsmethoden für Embeddings sowie die ihnen inhärenten Biase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AMup_-zoYuAY"
   },
   "source": [
    "## Aufgabe 1: Word2Vec CBOW\n",
    "> You shall know a word by the company it keeps.\n",
    ">\n",
    "> -- <cite>J. R. Firth</cite>\n",
    "\n",
    "Auf dem oben zitierten Prinzip beruhen die beiden als Word2Vec bekannt gewordenen Modelle CBOW und Skip Gram, die 2013 von [Tomas Mikolov et al.](https://arxiv.org/abs/1301.3781) bei Google entwickelt wurden.\n",
    "Erstgenanntes Modell werden wir im Folgenden in einer einfachen Form selbst implementieren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3xVNJ7AqYuAb"
   },
   "source": [
    "#### 1.1 Trainingsdaten\n",
    "Die Beschaffung und Aufbereitung von Trainingsdaten ist ein wichtiger Schritt in jeder NLP-Pipeline. Jetzt drücken wir uns mal davor und greifen auf einen Datensatz zu, den wir schonmal vorarb für Euch vorbereitet haben. Wir haben uns entschieden Alice im Wunderland [Gutenberg Project Alice im Wunderland](https://www.gutenberg.org/cache/epub/19778/pg19778.txt) zu nutzen um Word Embeddings zu trainieren. Der Datensatz beinhaltet die Sätze aus aus der Geschichte. \n",
    "\n",
    "Der vorverarbeitete Datensatz ist als pickle abgespeichert und findet sich in [hier](https://drive.google.com/file/d/1RfCivF-wHf33S7TMxjpT78923ADXreMi/view). Wir werden den googledrivedownloader nutzen, um die Datei zu laden. Ladet den Datensatz mit Pickle als Testdatensatz aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "_fgETXxtJ-p8"
   },
   "outputs": [],
   "source": [
    "#from google_drive_downloader import GoogleDriveDownloader as gdd\n",
    "\n",
    "#gdd.download_file_from_google_drive(file_id='1RfCivF-wHf33S7TMxjpT78923ADXreMi',\n",
    "#                                    dest_path='./download/alice_sentences.pkl',\n",
    "#                                    unzip=False,\n",
    "#                                    overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Bmu1bOjcYuAf"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(r\"alice_sentences.pkl\", \"rb\") as input_file:\n",
    "    data_sample = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eAjugIzMYuAq"
   },
   "source": [
    "### 1.2 Datenvorbereitung\n",
    "Wir haben einen tokenisierten Datensatz, der aus Listen von Wörtern besteht. Jede Liste repräsentiert einen Satz. Unser Modell soll aber hinterher mit Zahlen hantieren und zwar entweder mit Wortindizes, die jedes Wort im Vokabular über einen eindeutige Nummer referenzierbar machen, oder mit One-hot-Vektoren, die als Labels dienen, mit denen der tatsächliche Output des Modells verglichen werden kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "C3AIS0QfYuAt"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-8cc252d27e9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# um das Modell später weitertrainieren und die Embedding-Matrix interpretieren zu können.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Diese Datenstruktur kann, aber muss nicht, als Basis dienen.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0munique_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'der'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'hund'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'der'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bellt'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'die'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'katz'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'miaut'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Mapping von Wort zu ID\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "# Beim Mapping von Wörtern zu IDs und umgekehrt sollte eine reproduzierbare Reihenfolge sichergestellt werden,\n",
    "# um das Modell später weitertrainieren und die Embedding-Matrix interpretieren zu können.\n",
    "# Diese Datenstruktur kann, aber muss nicht, als Basis dienen.\n",
    "unique_words = OrderedDict.fromkeys(# Liste alle Wörter, die in unserem data_sample vorkommen) \n",
    "\n",
    "# Mapping von Wort zu ID\n",
    "word2id = # TODO\n",
    "\n",
    "# Mapping von ID zu Wort\n",
    "id2word = # TODO\n",
    "\n",
    "# Unser Data Sample, aber mit IDs statt Wörtern \n",
    "# [['der', 'hund', 'der', 'bellt'], ['die', 'katz', 'miaut']] => [[0, 1, 0, 2], [3, 4, 5]]\n",
    "numeric_docs = [[word2id[w] for w in doc] for doc in data_sample]\n",
    "\n",
    "print('Word to id sample:', list(word2id.items())[:10], '\\n')\n",
    "print('Id to word sample:', list(id2word.items())[:10], '\\n')\n",
    "print('Documents as lists of integers:', numeric_docs[0][:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UKXEBPLqYuA1"
   },
   "source": [
    "Wir halten einige wichtige Parameter für unser Modell fest. Die Größe des Kontextfensters sowie die Länge der Embeddingvektoren können nach Bedarf angepasst werden. Für unsere Demo wählen wir kleine Werte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ga_N0q-zYuA3"
   },
   "outputs": [],
   "source": [
    "vocabulary_size = # TODO\n",
    "embedding_size = 50 # Länge der Embeddingvektoren\n",
    "window_size = 2 # Größe des Kontextfensters. Wird nach rechts und links angewandt. Gesamter Kontext hier also 4 Wörter.\n",
    "\n",
    "print('Vocabulary Size:', vocabulary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uunq8JvnYuA-"
   },
   "source": [
    "### 1.3 Generator\n",
    "Um nicht alle Trainingsdaten auf einmal im Speicher halten zu müssen, schreiben wir uns eine Generatorfunktion, die Batches einer frei wählbaren Größe zurückgibt. Unser Ansatz ist dennoch nicht völlig speicherschonend, weil wir uns die Datengrundlage für die Generierung dieser Batches, nämlich die Integerlisten in `numeric_docs` sehr wohl im Speicher vorhalten. Darüber sehen wir aber großzügig hinweg.\n",
    "\n",
    "Die Generatorfunktion erzeugt zwei numpy-Arrays der Länge `batch_size`, von denen das eine Listen mit Indizes der Kontexwörter enthält, die der Embedding-Layer als Eingabe erwartet, und das andere die zugehörigen One-Hot-Encodings der Mittelwörter.\n",
    "\n",
    "Fiktives und vereinfachtes Beispiel:\n",
    "<pre><code>* Fenstergröße: 1\n",
    "* Batch-Size: 2\n",
    "* Wortindizes: 'die': 0, 'ente': 1, 'lacht': 2, 'und': '3, 'quakt': 4 (und damit Vokabulargröße 5)\n",
    "* Korpus (Auszug): [['die', 'ente', 'lacht', 'und', 'quakt'], ...]\n",
    "\n",
    "\n",
    "=> Rückgabe: [[0, 2], [1, 3]], [[0, 1, 0, 0, 0], [0, 0, 1, 0, 0]]\n",
    "</code></pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rItJIFM8YuBB"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "\n",
    "def generate_context_word_batches(corpus, window_size, vocab_size, batch_size):\n",
    "    X = []\n",
    "    Y = []\n",
    "    current_size = 0\n",
    "    while True:\n",
    "        # TODO: Here be dragons\n",
    "        yield contexts, label_words # zwei numpy arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zQU_48R7YuBG"
   },
   "outputs": [],
   "source": [
    "# Schneller Test\n",
    "test_batch_size = 3\n",
    "test_window_size = 2\n",
    "\n",
    "batch_gen = generate_context_word_batches(corpus=numeric_docs, window_size=test_window_size, vocab_size=vocabulary_size, batch_size=test_batch_size)\n",
    "for i in range(0, 3): \n",
    "    x, y = next(batch_gen)\n",
    "    for j in range(0, test_batch_size):\n",
    "        print('Context (X):', [id2word[w] for w in x[j]], '-> Target (Y):', id2word[np.argwhere(y[j])[0][0]])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jhy4rehNYuBM"
   },
   "source": [
    "### 1.4 Definition des Models\n",
    "Als nächstes definieren wir unser Model. Dazu verwenden wir die Sequential API von Keras. Dieser [Blogpost](https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html) bietet nochmal eine anschauliche Erklärung, wie word2vec CBOW funktioniert.\n",
    "Das folgende Bild ist daraus und stellt die Architektur des Neuronalen Netzes dar:\n",
    "<img src=\"https://lilianweng.github.io/lil-log/assets/images/word2vec-cbow.png\" width=500 />\n",
    "\n",
    "Weitere Informationen über den Embedding Layer finden sich [hier](https://keras.io/layers/embeddings/).\n",
    "\n",
    "> **Tipp:** Nehmt euch wirklich Zeit Embeddings zu verstehen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wdsQV9RDYuBO"
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Lambda\n",
    "\n",
    "#Modelldefinition\n",
    "cbow = Sequential()\n",
    "cbow.add(Embedding(input_dim=#TODO, output_dim=#TODO, input_length=#TODO))\n",
    "cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(embed_size,)))\n",
    "cbow.add(Dense(#TODO: #Units, activation=#TODO))\n",
    "cbow.compile(loss=#TODO, optimizer='rmsprop')\n",
    "\n",
    "# Zusammenfassung\n",
    "print(cbow.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KBoEx-w2YuBU"
   },
   "source": [
    "### 1.5 Training\n",
    "Jetzt wird es ernst: Wir trainieren unser Modell.\n",
    "Da wir eine eigene Generatorfunktion verwenden, müssen wir `steps_per_epoch` angeben. Überlegt euch was damit genau gemeint ist, wofür uns das nützt und was wir bei der Berechnung beachten müssen. Tipp: das Modell sollte pro Epoche alle Trainingsdaten sehen. Überlegt auch ob die Cbow-Fenstergröße einen einfluss auf diese Anzahl hat.\n",
    "\n",
    "Weil wir unser Modell gerne abspeichern möchten, zum Beispiel, um es später weiter zu trainieren, definieren wir eine Callback-Funktion, die das für uns übernimmt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Ui-skXyYuBV"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "model_checkpoint = ModelCheckpoint('embeddings.hd5', monitor='loss', verbose=1, save_best_only=True, save_weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lRuQzCB2YuBa"
   },
   "outputs": [],
   "source": [
    "epochs = 5 #(kann gerne erhöht werden)\n",
    "batch_size = 300\n",
    "# das Modell sollte pro Epoche alle Trainingsdaten sehen. Überlegt auch ob die Cbow-Fenstergröße einen einfluss auf diese Anzahl hat.\n",
    "steps_per_epoch = #TODO\n",
    "\n",
    "\n",
    "cbow.fit_generator(# TODO, callbacks=[#TODO])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SMIqy_-fYuBf"
   },
   "source": [
    "### 1.6 Test\n",
    "Nachdem wir unser Modell nur sehr kurz und nur auf wenigen Daten trainiert haben, ist davon auszugehen, dass die Ergebnisse nicht optimal sind. Einen kurzen Blick wollen wir dennoch riskieren.\n",
    "\n",
    "Dazu extrahieren wir zunächst die Gewichte aus dem Embedding-Layer und schauen sie uns auszugsweise an."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m7x0IPFqYuBh"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keras.models import load_model\n",
    "\n",
    "cbow = # TODO: Model laden\n",
    "embedding_weights = # TODO: Auf Embedding Layer (1. Layer) des Modells zugreifen und dort die Gewichtsmatrix extrahieren\n",
    "\n",
    "pd.DataFrame(# TODO: Was wollen wir anschauen?, index=list(id2word.values())).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5BnUH3l6YuBm"
   },
   "source": [
    "Da durch scharfes Hinsehen nicht unmittelbar zu erkennen ist, wie gut unsere Embeddings schon sind, machen wir stichprobenartige Tests. Dazu wählen wir einige Wörter und berechnen für deren Embeddings die Ähnlichkeit mit allen anderen Embedding-Vektoren in unserer Gewichtsmatrix. Anschließend lassen wir uns die fünf ähnlichsten Wörter ausgeben.\n",
    "Überlegt euch welches Distantzmaß für den Vergleich von Vektoren genutzt werden kann. Tipp: Die Cosinusähnlichkeit könnte damit was zu tun haben. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lwHpAG9lYuBo"
   },
   "outputs": [],
   "source": [
    "#Tipp: from sklearn.metrics.pairwise import ?\n",
    "\n",
    "sample_terms = ['Alice', 'Hut', 'Kaninchen','Kaninchenbau']\n",
    "sample_embeddings = # TODO\n",
    "\n",
    "# Berechne die paarweisen Distanzen zwischen Beispielwörtern und Gesamtvokabular\n",
    "distance_matrix = # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E1HEcW_bYuBt"
   },
   "outputs": [],
   "source": [
    "# Zeige die top fünf ähnlichsten Wörter zu unseren Beispielwörtern \n",
    "similar_words = {sample_term: [id2word[idx] for idx in distance_matrix[index].argsort()[1:6]] \n",
    "                   for index, sample_term in enumerate(sample_terms)}\n",
    "\n",
    "similar_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LM3qNiNwHyFl"
   },
   "source": [
    "Seid ihr zufrieden mit den ähnlichen Wörtern? Woran kann es liegen, dass die Wörter nicht immer unbedingt Sinn ergeben?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zt-nOPurYuBz"
   },
   "source": [
    "## Aufgabe 2: Gensim als Wrapper für Word2Vec-Modelle\n",
    "Embedding-Layer begegnen einem in der Praxis in der Tat häufig. In der Regel aber nicht als Bestandteile von reinen Word Embedding-Trainingsmodellen, sondern als erster Layer für Modelle mit anderen Aufgaben. Die Embeddings werden dann entweder mit vorberechneten Werten initalisiert und/oder werden im Training des Modells für die Downstream-Aufgabe (Textklassifikation, Übersetzung, ...) mittrainiert.\n",
    "\n",
    "Eine komfortable Möglichkeit, eigene Word2Vec-Modelle zu trainieren, bietet [Gensim](https://radimrehurek.com/gensim/models/word2vec.html), eine Bibliothek, die für diese Modelle auch Wrapper bereitstellt, um komfortabler an die Embeddings zu kommen und mit diesen zu arbeiten.\n",
    "\n",
    "Wir wollen uns im Folgenden einen kleinen Ausschnitt der Möglichkeiten, die Gensim bietet, anschauen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q86D9vsXFN2e"
   },
   "source": [
    "### 2.0 Vorbereitung\n",
    "Wir werden mit vortrainierten Google-News-Embeddings arbeiten, die ihr mit dem Code in den nächsten beiden Zellen herunterladen könnt. Falls das nicht funktionieren sollte, findet ihr die vortrainierten Embeddings [hier](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WXAQA-n5FgsQ"
   },
   "outputs": [],
   "source": [
    "!pip install googledrivedownloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hO0e4QiDEeUm"
   },
   "outputs": [],
   "source": [
    "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
    "\n",
    "gdd.download_file_from_google_drive(file_id='1Fl11N_cX1RfJmTHV1C-RGIsjWeIZjbTN',\n",
    "                                    dest_path='./download/GoogleNews-vectors-negative300.bin.gz',\n",
    "                                    unzip=False,\n",
    "                                    overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RTpLcc8yYuB0"
   },
   "source": [
    "### 2.1 Word2Vec-Model laden\n",
    "Die Vektoren haben eine Länge vin 300.\n",
    "Zieht euch die Embeddings über den oben angegebenen Link und verwendet gensim, um sie anschließend zu laden.\n",
    "\n",
    "**Hinweis**: Es kann einen Moment dauern, bis das Dictionary, das von Wort auf Embedding abbildet, erzeugt ist. Im Zweifel ein ```limit``` angeben und nur die ersten 1,5 Mio. Embeddings laden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uiXE550K0CfC"
   },
   "outputs": [],
   "source": [
    "!gunzip ./download/GoogleNews-vectors-negative300.bin.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Em5AdDKlYuB2"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "embeddings = # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wk7S_3n4YuB6"
   },
   "source": [
    "Aus Spaß an der Freude können wir nun schauen, wie gut unser trainiertes Modell ist bzw. ob es bestimmte von Menschen wahrgenommene Analogien bestätigt. \n",
    "\n",
    "Die Analogien sind [hier](https://github.com/nicholas-leonard/word2vec/blob/master/questions-words.txt) zu finden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r89GRNDrYuB6"
   },
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "\n",
    "embeddings.evaluate_word_analogies(datapath(\"questions-words.txt\"), restrict_vocab=30000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HCI5ETPYYuB_"
   },
   "source": [
    "### 2.2 Spaß mit Semantik\n",
    "Im Folgenden wollen wir uns mit dem Mehrwert beschäftigen, den semantische Embeddings bieten. Für weitere Inspiration siehe zum Beispiel [hier](https://www.machinelearningplus.com/nlp/gensim-tutorial/) und die [Doku](https://radimrehurek.com/gensim/models/keyedvectors.html).\n",
    "\n",
    "Mit semantischen Vektoren lassen sich zum Beispiel folgende Fragen beantworten:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1mgxhguPYuCA"
   },
   "outputs": [],
   "source": [
    "# Welche Stadt ist das New York Deutschlands? (Hinweis: 'New_York' ist als Token in den Embeddings enthalten)\n",
    "print('Das deutsche New York ist: {}\\n'.format(# TODO)))\n",
    "\n",
    "# Was ist Emacs besonders ähnlich?\n",
    "print('Ähnlichste Begriffe zu \"Emacs\": {}\\n'.format(# TODO))\n",
    "\n",
    "# Und wie sieht es mit Vim aus?\n",
    "print('Ähnlichste Begriffe zu \"Vim\": {}\\n'.format(# TODO))\n",
    "\n",
    "# Wer ist eigentlich der Mozart der Naturwissenschaft?\n",
    "print('Der Mozart der Naturwissenschaft ist: {}\\n'.format(# TODO))\n",
    "\n",
    "# Welches Wort verhält sich zu 'singing' wie 'burnt' zu 'burning'?\n",
    "print('burning:burnt wie singing:{}\\n'.format(# TODO))\n",
    "\n",
    "# Sind sich Deutschland und Frankreich ähnlicher oder Deutschland und Kanada?\n",
    "print('Ähnlichkeit DE, FR: {}'.format(# TODO))\n",
    "print('Ähnlichkeit DE, CAN: {}'.format(# TODO))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l3g5ivIxYuCE"
   },
   "source": [
    "Bei der Interpretation der Ergebnisse ist jedoch Vorsicht geboten: Es werden zwar semantische Beziehungen abgebildet, aber die entsprechen möglicherweise nicht immer den Erwartungen.\n",
    "\n",
    "Wie ähnlich sind sich zum Beispiel \"Leben\" und \"Tod\", \"kalt\" und \"warm\", \"Norden\" und \"Süden\"?\n",
    "\n",
    "Sind die Ergebnisse wie erwartet? Warum (nicht)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DXgw0wIMYuCF"
   },
   "outputs": [],
   "source": [
    "# TODO: Ähnlichkeiten berechnen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DPjd7_zJYuCJ"
   },
   "source": [
    "Die Embeddings sind nicht neutral, sondern spiegeln die Beziehungen wieder, die sich in den Trainingsdaten finden lassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LLI9StHLYuCK"
   },
   "outputs": [],
   "source": [
    "# Wird Wissenschaft von Frauen oder Männern gemacht?\n",
    "print('Wissenschaft wird gemacht von: {}'.format(# TODO))\n",
    "# Sind Mörder eher Schwarze, Weiße oder Asiaten?\n",
    "print('Mörder sind: {}'.format(# TODO))\n",
    "# Was bleibt vom Mann, wenn die Intelligenz abgezogen wird?\n",
    "print('Mann ohne Intelligenz: {}'.format(# TODO))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "05_1_text_representation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
