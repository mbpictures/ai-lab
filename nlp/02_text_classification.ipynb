{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "colab": {
   "name": "WS21_05_2_text_classification.ipynb",
   "provenance": [],
   "collapsed_sections": []
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "URsEpKN3YYXF"
   },
   "source": [
    "# Text Klassifikation - Sentiment Analyse\n",
    "\n",
    "In diesem Notebook möchten wir uns mit der Klassifikation von Texten beschäftigen. Vereinfacht gesagt beschäftigt sich Sentiment Analysis damit, natürlichsprachliche Aussagen dahingehend zu bewerten, ob die subjektive Aussage des Sprechers positiv oder negativ wertend gemeint ist.\n",
    "\n",
    "Zu diesem Zweck haben wir den Datensatz von _Sentiment140_, einem Projekt der Stanford University, ausgewählt. Er beinhaltet 16 Millionen Tweets, die aufgrund der enthaltenen Emoticons automatisch in positiv und negativ eingeteilt wurden.\n",
    "\n",
    "---\n",
    "\n",
    "Dieses Notebook gliedert sich in die folgenden Teile:\n",
    "\n",
    "1. Datenanalyse und Preprocessing\n",
    "2. Tokenisierung & Vokabular\n",
    "3. Klassifikation\n",
    "4. Erweiterung der Klassifikation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "baoj_cXFHCqr",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "outputId": "4d281c7f-3e42-4863-8397-708968e1430d"
   },
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "'2.6.0'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZLSIuUHq-qc_"
   },
   "source": [
    "## 0. Vorbereitung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xyE-j8e1_qTw"
   },
   "source": [
    "Als erstes muss der Datensatz von http://help.sentiment140.com/for-students in diese Colab Virtual Machine geladen werden."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qzK4XN9vBBWz",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "e00f5fd7-3d04-46b2-8b4c-00aae353cc31"
   },
   "source": [
    "!pip install googledrivedownloader"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: googledrivedownloader in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.4)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NY_uOpPsAfSQ",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "84080bc9-0cfa-46c4-bf4e-13fc02df4cbd"
   },
   "source": [
    "#from google_drive_downloader import GoogleDriveDownloader as gdd\n",
    "\n",
    "#gdd.download_file_from_google_drive(file_id='0B04GJPshIjmPRnZManQwWEdTZjg',\n",
    "#                                    dest_path='./download/trainingandtestdata.zip',\n",
    "#                                    unzip=False,\n",
    "#                                    overwrite=True)\n",
    "\n",
    "#DATA_DIR = 'download/sentiment140'"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "87ZXB6FoBV_y",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "806b60b9-fb23-4c5f-d7f0-58f3d9820de2"
   },
   "source": [
    "#!unzip download/trainingandtestdata.zip -d {DATA_DIR}\n",
    "#%rm download/trainingandtestdata.zip\n",
    "#%ls -la {DATA_DIR}"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O-W8Qq9m-YPu"
   },
   "source": [
    "## 1. Datenbeschaffung und -Analyse\n",
    "\n",
    "Im ersten Schritt sollen folgende Schritte durchgeführt werden.\n",
    "\n",
    "1. Ladet den Datensatz in einen Pandas Dataframe. Welche Feature gibt es? Wie viele Samples gibt es?\n",
    "\n",
    "2. Da wir uns nur für die Felder `polarity` und `text` interessieren, sollte der Frame mit den Daten folgendes Format haben : `id => (polarity, text)`. \n",
    "\n",
    "3. Wandelt die Polarity Werte in 1 (positiv) und 0 (negativ) um.\n",
    "\n",
    "4. Fügt eine Spalte für die Anzahl an Wörtern im Text hinzu. Was ist die durschnittliche Tweetlänge\n",
    "\n",
    "5. Analysiert den Datensatz mit beliebigen weiteren Pandas Boardmitteln."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7YpZdpCzi79l"
   },
   "source": [
    "import pandas as pd"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "RRWljU0LYYXJ"
   },
   "source": [
    "df = pd.read_csv(\n",
    "    \"training.1600000.processed.noemoticon.csv\",\n",
    "    encoding=\"ISO-8859-1\",\n",
    "    header=None,\n",
    "    names=[\"polarity\", \"id\", \"date\", \"query\", \"user\", \"text\"]\n",
    ")"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4eF_dIHzJK1_"
   },
   "source": [
    "df = df[[\"polarity\", \"id\", \"text\"]]"
   ],
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3nzSQ3ouJOib"
   },
   "source": [
    "di = {0: 0, 4: 1}\n",
    "df = df[df[\"polarity\"] != 2].replace({\"polarity\": di})"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TTyMBU61AY7a"
   },
   "source": [
    "df['word_count'] = df.apply(lambda row: len(row.text), axis=1)\n",
    "print(f'Mean tweet length: {df.word_count.mean()}')"
   ],
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean tweet length: 74.09011125\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "             count       mean        std  min   25%   50%    75%    max\npolarity                                                               \n0         800000.0  74.301790  36.743260  6.0  44.0  70.0  104.0  359.0\n1         800000.0  73.878433  36.135274  6.0  44.0  69.0  103.0  374.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>count</th>\n      <th>mean</th>\n      <th>std</th>\n      <th>min</th>\n      <th>25%</th>\n      <th>50%</th>\n      <th>75%</th>\n      <th>max</th>\n    </tr>\n    <tr>\n      <th>polarity</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>800000.0</td>\n      <td>74.301790</td>\n      <td>36.743260</td>\n      <td>6.0</td>\n      <td>44.0</td>\n      <td>70.0</td>\n      <td>104.0</td>\n      <td>359.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>800000.0</td>\n      <td>73.878433</td>\n      <td>36.135274</td>\n      <td>6.0</td>\n      <td>44.0</td>\n      <td>69.0</td>\n      <td>103.0</td>\n      <td>374.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"polarity\").describe()[\"word_count\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nVZCgmAnYYXV"
   },
   "source": [
    "## 2. Tokenisierung\n",
    "\n",
    "Um später ein Modell zur Sentiment Analyse trainieren zu können, sollen die Daten nun aufbereitet werden. Da die sinntragenden Elemente in den Tweets die Wörter sind, sollten die Tweets in Wörter aufgeteilt werden. Um genau zu sein, ist der Term 'Wörter' hier aus linguistischer Sicht etwas falsch, man spricht eigentlich von Tokens. Daher nennt man das Aufteilen von Text auch Tokenizing und die Funktion, die sowas kann, Tokenizer.\n",
    "\n",
    "1. Der allereinfachste Tokenizer ist vermutlich die `split` Methode. Tokenisiert damit die eingelesen Tweets. Am Ende solltet ihr eine Liste `df['tokenized'] = [token_1,token_2, ...]` für jedes Sample erhalten."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "esbcYU6hYYXX"
   },
   "source": [
    "df['tokenized'] = df.apply(lambda row: row.text.split(\" \"), axis=1)"
   ],
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U2V0tecoYYXe"
   },
   "source": [
    "Abgesehen von natürlichsprachlichen Wörtern sind in Tweets mindestens auch Hashtags, Mentions und Links enthalten. Überlegt euch, ob es Sinn ergibt, alle diese Bestandteile in den Daten in dieser Form zu behalten. Begründet kurz eure Entscheidungen.\n",
    "\n",
    "Falls ihr euch entschlossen habt, nicht alle diese Bestandteile zu behalten, filtert dementsprechend eure Daten. Die Struktur der Daten sollte am Ende gleich bleiben: `df['cleaned'] = [token_1,...]`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "31WRx4aKYYXh"
   },
   "source": [
    "import validators\n",
    "\n",
    "def clean(token):\n",
    "    if token.startswith(\"@\"):\n",
    "        return \"<MENTION>\"\n",
    "    if token.startswith(\"#\"):\n",
    "        return \"<HASHTAG>\"\n",
    "    if token == \"&amp;\":\n",
    "        return \"and\"\n",
    "    if token.startswith(\"http\") and validators.url(token):\n",
    "        return \"<LINK>\"\n",
    "    return token\n",
    "\n",
    "def filter_token(token):\n",
    "    if token == \"\" or token == \"-\":\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "df['cleaned'] = df.apply(lambda row: [clean(token) for token in row.tokenized if filter_token(token)], axis=1)"
   ],
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QH2E049sYYXn"
   },
   "source": [
    "Zählt die Tokens in eurem Datensatz. Benutzt dafür ein Dictionary oder andere Python Build-Ins. \n",
    "\n",
    "1. Wieviele unterschiedliche Wörter gibt es?\n",
    "2. Gebt die 100 häufigsten Wörter sortiert aus. \n",
    "\n",
    "Was zieht ihr aus den beiden Analysen? Was müsst ihr zusätzlich noch filtern?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "66v12PBQYYXo"
   },
   "source": [
    "tokens = dict()\n",
    "for index, row in df.iterrows():\n",
    "    for token in row.cleaned:\n",
    "        if not token in tokens:\n",
    "            tokens[token] = 0\n",
    "        tokens[token] += 1\n",
    "        \n",
    "tokens_sorted = sorted(tokens.items(), key=lambda item: item[1], reverse=True)\n",
    "print(tokens_sorted[:100])\n"
   ],
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<MENTION>', 793948), ('to', 552962), ('I', 496608), ('the', 487500), ('a', 366212), ('and', 315391), ('my', 280025), ('i', 249975), ('is', 217692), ('you', 213871), ('for', 209800), ('in', 202294), ('of', 179554), ('it', 171810), ('on', 154365), ('have', 132249), ('so', 125154), ('me', 122509), ('that', 118684), ('with', 110843), ('be', 108069), ('but', 106271), ('at', 102196), (\"I'm\", 99559), ('was', 99140), ('just', 96284), ('not', 88110), ('this', 77809), ('get', 76733), ('like', 73302), ('are', 72568), ('<LINK>', 70538), ('up', 70007), ('all', 67901), ('out', 67030), ('go', 62969), ('your', 60854), ('good', 59775), ('day', 55748), ('do', 54628), ('from', 54182), ('got', 53870), ('now', 53591), ('going', 53236), ('love', 50051), ('no', 49621), ('about', 46708), ('work', 45913), ('will', 45898), ('<HASHTAG>', 44108), ('back', 44033), ('u', 43566), (\"it's\", 43422), ('some', 42745), ('am', 42724), ('can', 42506), (\"don't\", 42472), ('really', 42152), ('had', 41548), ('see', 41342), ('know', 41338), ('one', 41082), ('too', 40634), ('time', 39570), ('we', 39438), ('want', 39218), ('what', 38787), ('im', 37599), ('think', 37355), ('as', 36423), (\"can't\", 36356), ('new', 35590), ('when', 35440), ('its', 35196), ('still', 35054), ('2', 35005), ('if', 34779), ('an', 34074), ('miss', 33135), ('today', 32923), ('more', 32697), ('need', 31898), ('how', 31367), ('last', 31186), ('has', 30588), ('been', 30498), ('they', 29601), ('My', 29404), ('much', 28980), ('feel', 27961), ('or', 27837), ('lol', 27676), ('off', 27412), (\"i'm\", 27237), ('home', 27116), ('Just', 27067), ('then', 26882), ('The', 26831), ('her', 26520), ('there', 26389)]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pm76wuIZzhGC"
   },
   "source": [
    "Eine Möglichkeit für komplexere Preprocessing-Methoden ist das Entfernen von Stoppwörtern. Hiefür nutzen wir [NLTK](https://www.nltk.org/).\n",
    "\n",
    "Filtert eure gesäuberten Tokens auf Stopwörter."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "N4K-ZoYRzf2B"
   },
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "df['cleaned'] = df.apply(lambda row: [token for token in row.cleaned if not token in stopwords], axis=1)"
   ],
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1BSWVNV0YYXu"
   },
   "source": [
    "## 3. Klassifikation\n",
    "\n",
    "Wie Eingangs erwähnt, beschäftigt sich Sentiment Analysis damit, eine Äußerung automatisch dahingehend zu klassifizieren,\n",
    "ob der Inhalt positiv oder negativ gemeint ist.\n",
    "Es handelt sich also um eine binäre Klassifikation.\n",
    "\n",
    "Für das Training des neuronalen Netzes orintiert sich der nachfolgende Teil an _Keras_ als Framework. Ihr könnt aber auch ein anderes Framework wie bspw. _Pytorch_ benutzen. \n",
    "\n",
    "Aktuell liegen unsere Daten zwar in tokenisierter und gesäuberter Form vor, wir müssen unsere Daten aber noch in Vektoren transformieren.\n",
    "\n",
    "Für die erste Klassifikation encoden wir die Eingabe als **Bag-of-Words**, sodass jedes potentielle Wort einem Eingabeneuron einspricht. _Ein Beispiel_: Zwei Tweets \"lorem ipsum\" und \"foo foo bar\", die Vektoren hätten die Länge 4 und für den ersten Tweet wäre der Vektor [1,1,0,0], für den zweiten [0,0,2,1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pIh-IUrCetfp"
   },
   "source": [
    "### 3.1 Vokabular\n",
    "\n",
    "Befüllt das dictionary `word2idx` so, dass jedes Wort auf einen Index abgebildet wird und die Indizes streng monoton aufsteigend sind. Für das Beispiel oben wäre `word2idx = {\"lorem\": 0, \"ipsum\": 1, \"foo\": 3, \"bar\": 4}`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "aj0I7deSYYXx"
   },
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "word2idx = dict()\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    for token in row.cleaned:\n",
    "        if token in word2idx: continue\n",
    "        word2idx[token] = index"
   ],
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1600000/1600000 [01:34<00:00, 17005.28it/s]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wShRcQcfYYX2"
   },
   "source": [
    "Welche Länge werden die Vektoren haben?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "eM8DyIgNYYX4"
   },
   "source": [
    "VECTOR_LEN = len(word2idx)\n",
    "print(VECTOR_LEN)"
   ],
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "908070\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fc65A8wwYYX9"
   },
   "source": [
    "Wir könnten mit `numpy` ein Array befüllen, das für jeden der 16 Millionen Tweets einen Vektor wie oben beschrieben enthält. \n",
    "Bevor ihr damit beginnen, überschlagt, wieviel Speicherplatz (im Hauptspeicher) ein solches Array belegen würde, wenn jeder Eintrag 32 bit hat. Reicht euer Hauptspeicher dafür aus?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "aYODrE0LYYX_"
   },
   "source": [
    "MEMORY = 16000000 * 4 * VECTOR_LEN # 32 bit = 4byte\n",
    "print(MEMORY / 1024 / 1024 / 1024) # GiB"
   ],
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54125.189781188965\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iGTNZMlIYYYD"
   },
   "source": [
    "### 3.2 Data Generator\n",
    "Um das Problem mit dem zu kleinen Hauptspeicher zu umgehen, bietet Keras die Möglichkeit, anstatt auf einem kompletten Datensatz zu operieren, immer nur kleinere Häppchen abzuarbeiten. Dazu wird ein Python-Generator eingesetzt.\n",
    "Vervollständigt die Funktion unten, so dass ein Generator entsteht. Die Parameter der Funktion sind:\n",
    " * x: tokenisierte und gesäuberte Tweets\n",
    " * y: korrespondierende Labels von x\n",
    " * w2i: das word2index dictionary\n",
    " * batch_size: Anzahl der vektorisierten Tweets, die pro Aufruf zurückgegeben werden sollen.\n",
    " \n",
    "Die benutzen Tweets werden nacheinander aus `(x,y)` ausgewählt und kein Tweet darf mehrfach zurückgegeben werden."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mRVPs_ZlYYYF"
   },
   "source": [
    "from math import ceil\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def data_generator(x, y, w2i, batch_size):\n",
    "  num_batches = ceil(len(x) / batch_size)\n",
    "  current_batch = 0\n",
    "\n",
    "  while True:\n",
    "    # Initialize with zeros\n",
    "    batch_x = np.zeros((batch_size, len(w2i.keys())))\n",
    "    batch_y = np.zeros((batch_size, 1))\n",
    "\n",
    "    current_entry = 0\n",
    "    for i in range(current_batch * batch_size, (current_batch + 1) * batch_size):\n",
    "        for token in x[i]:\n",
    "            batch_x[current_entry][w2i[token]] += 1\n",
    "        batch_y[current_entry] = [y[i]]\n",
    "        current_entry += 1\n",
    "  \n",
    "    current_batch += 1\n",
    "    yield batch_x, batch_y\n",
    "\n",
    "    # Reset counter\n",
    "    if current_batch >= num_batches:\n",
    "      current_batch = 0"
   ],
   "execution_count": 29,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ETv9mEIhYYYM"
   },
   "source": [
    "gen = data_generator(df[\"cleaned\"].to_numpy(), df[\"polarity\"].to_numpy(), word2idx, 256) # TODO"
   ],
   "execution_count": 30,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VwLI3pTcYYYK"
   },
   "source": [
    "Ihr könnt euren Generator wie folgt ausprobieren:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JstM-btxKKDv"
   },
   "source": [
    "next(gen)"
   ],
   "execution_count": 38,
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.73 GiB for an array with shape (256, 908070) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mMemoryError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_1440/783752693.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mnext\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mgen\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_1440/2294667448.py\u001B[0m in \u001B[0;36mdata_generator\u001B[1;34m(x, y, w2i, batch_size)\u001B[0m\n\u001B[0;32m      9\u001B[0m   \u001B[1;32mwhile\u001B[0m \u001B[1;32mTrue\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     10\u001B[0m     \u001B[1;31m# Initialize with zeros\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 11\u001B[1;33m     \u001B[0mbatch_x\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mzeros\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbatch_size\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mw2i\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeys\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     12\u001B[0m     \u001B[0mbatch_y\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mzeros\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbatch_size\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mMemoryError\u001B[0m: Unable to allocate 1.73 GiB for an array with shape (256, 908070) and data type float64"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b1qy5nlEYYYW"
   },
   "source": [
    "### 3.3 Neuronales Netz\n",
    "\n",
    "Wir sind nun endlich soweit, unser neuronales Netz aufzubauen. Da unser Netz genau ein hidden Layer hat und auch sonst nicht sonderlich komplex ist, benutzen wir die _Sequential_-API von Keras."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VY_sZSc1YYYX"
   },
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "m = Sequential([\n",
    "    tf.keras.Input(shape=(VECTOR_LEN,))\n",
    "])"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rcE9-qJxYYYc"
   },
   "source": [
    "Fügt einen _Dense_-Layer dem Netz hinzu, als _hidden units_ könnt ihr 16 nehmen. Da dies auch der Eingabe-Layer ist, müsst ihr den Parameter `input_shape` definieren. (Siehe auch: https://keras.io/layers/core/)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "uWkgD9ACYYYd"
   },
   "source": [
    "m.add(tf.keras.layers.Dense(16, activation=\"relu\")) # TODO"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LnqAOXj5YYYi"
   },
   "source": [
    "Als letzten Layer in unserem neuronalen Netz, fügt einen weiteren _Dense_-Layer hinzu. Dieser Layer dient auch als \"Ausgabelayer\" Überlegt euch die Anzahl der _hidden units_ (Hinweis: Wie lässt sich unser Machine-Learning-Problem kategorisieren?) Welche _Activation_-Funktion wählt ihr?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-EdR3LgfYYYj"
   },
   "source": [
    "m.add(tf.keras.layers.Dense(2, activation=\"softmax\"))"
   ],
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JlSaW4I1YYYn"
   },
   "source": [
    "Kompiliert das neuronale Netz. Als `optimizer` könnt ihr `adam` benutzen. Wählt eine passende `loss`-Funktion aus. Begründet eure Entscheidung. (https://keras.io/models/model/#compile)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0U54vEg-YYYo"
   },
   "source": [
    "m.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bpt8M2ynYYYs"
   },
   "source": [
    "### 3.3. Training\n",
    "\n",
    "Bevor ihr nun das neuronale Netz trainiert, teilt noch euren Datensatz in drei Teile auf. Einen Teil zum Trainieren, einen zum Evaluieren und einem zum Testen. Das Verhältnis der beiden Datensätze sollte 70%:20%:10% sein. Bevor ihr die Daten aufteilt, durchmischt sie mit der `shuffle`-Methode aus dem `random`-Modul. Außerdem solltet ihr die Datenmenge zunächst auf ca. 100000 begrenzen, damit das Training des neuronalen Netzes nicht ewig dauert."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0TLMbqBDYYYt"
   },
   "source": [
    "df_shuffeled = df.sample(frac=1).reset_index(drop=True).head(100000)\n",
    "df_train = df_shuffeled.loc[0:len(df_shuffeled) * 0.7]\n",
    "df_val = df_shuffeled.loc[len(df_shuffeled) * 0.7:len(df_shuffeled) * 0.9]\n",
    "df_test = df_shuffeled.loc[len(df_shuffeled) * 0.9:len(df_shuffeled)]\n",
    "\n",
    "assert((len(df_train) + len(df_val) + len(df_test)) == len(df_shuffeled))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "POelfhirYYYx"
   },
   "source": [
    "Wir sind nun soweit das neuronale Netz zu trainieren. Da wir den oben entwickelten Generator einsetzen wollen, verwenden wir dazu die `fit_generator`-Methode. Als `batch_size` könnt ihr 100 nehmen, für den `epochs`-Parameter 10. Was wählt ihr als `steps_per_epoch`-Parameter? Wie übergebt ihr die Validierungsdaten? (https://keras.io/models/model/#fit)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "i0sWka05YYYy"
   },
   "source": [
    "batch_size = 100\n",
    "\n",
    "m.fit(\n",
    "    data_generator(df_train.cleaned.to_numpy(), df_train.polarity.to_numpy(), word2idx, batch_size),\n",
    "    epochs=10,\n",
    "    steps_per_epoch=len(df_train) / batch_size,\n",
    "    validation_data=data_generator(df_val.cleaned.to_numpy(), df_val.polarity.to_numpy(), word2idx, batch_size)\n",
    ")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LvBjnS3YYYY2"
   },
   "source": [
    "Während das Netz trainiert wird, könnt ihr euch Gedanken zur Evaluierung machen:\n",
    "   * Definiert die üblichen Fehlerklassen (wahr positiv, falsch positiv, wahr negativ, falsch negativ)\n",
    "   * Eine häufig benutzte Evaluationsmetrik ist die _Accuracy_. Beschreibt diese Metrik und schreibt die Formel zur Berechnung auf.\n",
    "   * Warum könnte die _Accuracy_ eine schlechte Metrik sein?\n",
    "   * Zur Evaluation von binären Klassifikationsproblemen wird in der Literatur gerne _Precision_ und _Recall_ verwendet. Wie sind die beiden Evaluationsmaße definiert? Beschreibt diese Metriken mit eigenen Worten. Schreibt auch die Formeln zur Berechnung auf.\n",
    "   * Warum könnten _Precision_ und _Recall_ bessere Metriken sein als _Accuracy_?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sO7uif0sYYY4"
   },
   "source": [
    "11. Inzwischen sollte das Netz fertig trainiert sein. Speichert es ab!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MpOkWtFWYYY5"
   },
   "source": [
    "m.save('my_net.h5')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z_TJCJK9YYY8"
   },
   "source": [
    "### 3.4. Evaluation\n",
    "\n",
    "Evaluiert euer Netz mit dem Test-Datensatz, den ihr oben beseite gelegt haben. Benutzt dafür die `predict`-Methode des Models. Berechnet dafür _Precision_, _Recall_, _Accuracy_ und _Confusion Matrix_. \n",
    "\n",
    "Interpretiert kurz eure Ergebnisse. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "guhq9F_Kaq7d"
   },
   "source": [
    "# TODO"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gI0YZIpvYYZA"
   },
   "source": [
    "## _(Optional)_ 4. Erweiterung der Klassifikation\n",
    "Im nächsten Schritt wollen wir das Netz auf dem vollen Datensatz trainieren und die Komplexität etwas erhöhen.\n",
    "\n",
    "1. Trainiert euer Netz auf dem großen Datensatz. Was sind die Erkentnisse?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_4GkVcQRd4W2"
   },
   "source": [
    "# TODO"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bQ1XbUuJd5xd"
   },
   "source": [
    "2. Verändert die Parameter Ihres Netzes (z.B Anzahl _hidden units_, Anzahl _hidden layers_) und trainiert das Netz erneut (auf dem kleinen Datensatz). Was stellt ihr fest?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OoiplZnYd6Q8"
   },
   "source": [
    "# TODO"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}