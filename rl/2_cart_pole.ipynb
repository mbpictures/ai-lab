{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HSKA AI-Lab RL: Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mount Google Drive as folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "%cd /content/drive/My\\ Drive/ai-lab/rl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal-Difference Methoden\n",
    "\n",
    "In diesem Notebook geht es darum Q-Learning anhand des Beispiels [CartPole](https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py) zu implementieren. CartPole steht im OpenAI Gym zur Verfügung und wird in der Implementierung (`CartPoleEnv`) wie folgt beschrieben:\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "A pole is attached by an un-actuated joint to a cart, which moves along\n",
    "a frictionless track. The pendulum starts upright, and the goal is to\n",
    "prevent it from falling over by increasing and reducing the cart's\n",
    "velocity.\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CartPole Environment kennenlernen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install gym[atari]==0.12.5\n",
    "\n",
    "import sys\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from operator import itemgetter\n",
    "\n",
    "!apt-get install -y xvfb python-opengl\n",
    "%pip install pyvirtualdisplay\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from plot_utils import plot_values, visualize_samples, plot_statistics\n",
    "from abstract_agent import AbstractAgent\n",
    "import check_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ziel des Agenten ist es, den Pol so lange wie möglich aufrecht zu halten. Für jeden \"Zeitschritt\" den der Agent macht, bekommt dieser einen Reward von +1 (inkl. dem terminalen State). Die Episode terminiert, sobald der Polwinkel mehr als 12 Grad beträgt, die Wagenposition mehr als 2,4 (Mitte des Wagens erreicht den Rand der Anzeige) von der Mitte entfernt ist oder die Episodenlänge größer als 200 ist.\n",
    "\n",
    "Gelöst gilt das CartPole-Problem, wenn folgendes gilt: der durchschnittliche Reward über 100 aufeinanderfolgende Versuche ist größer oder gleich 195,0 ist.\n",
    "\n",
    "Der State setzt sich wie folgt zusammen:\n",
    "\n",
    "```\n",
    "Num\tObservation               Min             Max\n",
    "0\tCart Position             -4.8            4.8\n",
    "1\tCart Velocity             -Inf            Inf\n",
    "2\tPole Angle                -24 deg         24 deg\n",
    "3\tPole Velocity At Tip      -Inf            Inf\n",
    "```\n",
    "\n",
    "Zu Beginn jeder Episode wird allen Beobachtungen ein einheitlicher Zufallswert in [-0.05..0.05] zugewiesen. Der State-Space (siehe `observation_space` in der nachfolgenden Zelle) ist fortlaufen (continuous state space).\n",
    "\n",
    "Der Agent hat 2 mögliche Actions zur Auswahl, mit $\\mathcal{A} =\\{0, 1\\}$ (siehe `action_space` in der nachfolgenden Zelle).:\n",
    "```\n",
    "PUSH CART TO THE LEFT = 0\n",
    "PUSH CART TO THE RIGHT = 1\n",
    "```\n",
    "\n",
    "```\n",
    "Note: The amount the velocity that is reduced or increased is not\n",
    "fixed; it depends on the angle the pole is pointing. This is because\n",
    "the center of gravity of the pole increases the amount of energy needed\n",
    "to move the cart underneath it\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the action space\n",
    "print(\"Action space:\", env.action_space)\n",
    "\n",
    "# Generate some samples from the action space\n",
    "print(\"Action space samples:\")\n",
    "print(np.array([env.action_space.sample() for i in range(10)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der State-Space und damit auch die Policy können hier, aufgrund der kontinuierlichen Werte die ein State annehmen kann, nicht mehr so einfach visuell dargestellt werden wie wir das bei dem `CliffWalking` Environment gesehen haben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore state (observation) space\n",
    "print(\"State space:\", env.observation_space)\n",
    "print(\"- low:\", env.observation_space.low)\n",
    "print(\"- high:\", env.observation_space.high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some samples from the state space \n",
    "print(\"State space samples:\")\n",
    "print(np.array([env.observation_space.sample() for i in range(10)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nachfolgend wird ein Agent gezeigt, der zufällig über dem Action-Space sampled und die entsprechenden Actions ausführt. Den Score gilt es dann im Laufe des Notebooks zu verbessern und schließlich das Environment zu lösen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random agent\n",
    "\n",
    "# Set plotting options\n",
    "plt.style.use('ggplot')\n",
    "np.set_printoptions(precision=3, linewidth=120)\n",
    "\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()\n",
    "\n",
    "is_ipython = 'inline' in plt.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "env.seed(505);\n",
    "\n",
    "state = env.reset()\n",
    "img = plt.imshow(env.render(mode='rgb_array'))\n",
    "for t in range(1000):\n",
    "    action = env.action_space.sample()\n",
    "    img.set_data(env.render(mode='rgb_array')) \n",
    "    plt.axis('off')\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    if done:\n",
    "        print('Score: ', t+1)\n",
    "        break\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 2.1: Diskretisierung des State-Spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im vorherigen Abschnitt haben wir gesehen, dass der State-Space kontinuierlich ist und wir damit zu viele Zustände haben, um diese in einer einfachen Q-Tabelle zu speichern. Eine Möglichkeit, dieses Problem in den Griff zu bekommen, ist die Diskretisierung des State-Spaces, um die Q-Werte wieder tabellarisch ablegen zu können.\n",
    "\n",
    "Für das bessere Verständnis dafür, wie das hinterher aussehen soll, wird nachfolgend die Diskretisierung des State-Spaces anhand der Features `Cart Position` und `Cart Velocity` exemplarisch gezeigt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example data for visualization of disrectized cart features\n",
    "samples = np.array([[-2.875e+00, -2.926e+38],\n",
    "                    [-1.143e-01, -1.770e+38],\n",
    "                    [ 3.638e-01, -4.583e+37],\n",
    "                    [ 3.908e+00,  8.961e+37],\n",
    "                    [ 3.913e+00, -3.193e+38],\n",
    "                    [-7.072e-01, -8.241e+37],\n",
    "                    [ 1.191e+00,  3.233e+38],\n",
    "                    [-3.152e+00, -1.207e+38],\n",
    "                    [-1.267e+00,  3.655e+37],\n",
    "                    [ 1.716e+00, -1.660e+37]])\n",
    "discretized_samples = np.array([[2, 0],\n",
    "                                [4, 2],\n",
    "                                [5, 4],\n",
    "                                [9, 6],\n",
    "                                [9, 0],\n",
    "                                [4, 3],\n",
    "                                [6, 9],\n",
    "                                [1, 3],\n",
    "                                [3, 5],\n",
    "                                [6, 4]])\n",
    "grid = [np.array([-3.84, -2.88, -1.92, -0.96,  0.  ,  0.96,  1.92,  2.88,  3.84]),\n",
    "        np.array([-2.722e+38, -2.042e+38, -1.361e+38, -6.806e+37,  0.000e+00,  6.806e+37,  1.361e+38,  2.042e+38,  2.722e+38])]\n",
    "low = [-4.800e+00, -3.403e+38]\n",
    "high = [4.800e+00, 3.403e+38]\n",
    "\n",
    "visualize_samples(samples, discretized_samples, grid, low, high,\n",
    "                  x_label='Cart position', y_label='Cart velocity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir werden den State-Space durch ein gleichmäßiges Raster diskretisieren. Implementieren Sie die folgende Methode, um ein solches Gitter unter Berücksichtigung der unteren Grenzen (`low`), der oberen Grenzen (`high`) und der Anzahl der gewünschten `bins` entlang jeder Dimension zu erstellen. Die Methode sollte die Aufteilungspunkte für jede Dimension zurückgeben. Die Anzahl der Aufteilungspunkte ist um 1 kleiner als die Anzahl der Bins.\n",
    "\n",
    "Zum Beispiel, wenn `low = [-1.0, -5.0]`, `high = [1.0, 3.0]` und `bins = (10, 10)`, dann sollte die Methode folgende `list` von zwei NumPy Arrays zurückgeben:\n",
    "\n",
    "```\n",
    "[array([-0.8, -0.6, -0.4, -0.2,  0.0,  0.2,  0.4,  0.6,  0.8]),\n",
    " array([-4.0, -3.0, -2.0, -1.0,  0.0,  1.0,  2.0,  3.0,  4.0])]\n",
    "```\n",
    "\n",
    "Dieses erzeuget Grid wird dann verwendet, um die kontinuierlichen Werte des Environments konkreten Zellen in dem Grid zuzuweisen und damit den State-Space zu vereinfachen.\n",
    "\n",
    "**Wichtig**: Die Randwerte von `low` und `high` sind **nicht** in den Aufteilungspunkten enthalten. Es wird angenommen, dass jeder Wert unterhalb des niedrigsten Aufteilungspunktes dem Index `0` und jeder Wert oberhalb des höchsten Aufteilungspunktes dem Index `n-1` zugeordnet wird, wobei `n` die Anzahl der Behälter entlang dieser Dimension ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_uniform_grid(low, high, bins=(10, 10)):\n",
    "    \"\"\"Create a uniformly-spaced grid that can be used to discretize a space.\n",
    "    \n",
    "    Args:\n",
    "        low [array]: Lower bounds for each dimension of the continuous space.\n",
    "        high [array]: Upper bounds for each dimension of the continuous space.\n",
    "        bins [tuple]: Number of bins along each corresponding dimension.\n",
    "    \n",
    "    Returns:\n",
    "        grid [list]: List of arrays containing split points for each dimension.\n",
    "    \"\"\"\n",
    "    assert len(low) == len(high), 'Dimensions of low and high should be equal'\n",
    "    # TODO\n",
    "    return []\n",
    "\n",
    "# Test implementation\n",
    "low = [-1.0, -5.0]\n",
    "high = [1.0, 5.0]\n",
    "create_uniform_grid(low, high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assertions, please do not change\n",
    "assert len(create_uniform_grid([0, 0, 0], [1, 1, 1], (1, 1, 1))) == 3, 'Grid shape not matching input values. Please check implementation'\n",
    "assert np.allclose(create_uniform_grid([-42., -3.14], [0.1, 0.11], (3, 5))[1], np.array([-2.49, -1.84, -1.19, -0.54])), 'Grid is not as expected. Please check implementation'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schreiben Sie nun eine Methode, die Samples aus einem kontinuierlichen State-Space in seine äquivalente diskretisierte Darstellung umwandeln kann, wobei ein Grid wie das oben erstellte vorgegeben wird. Zu diesem Zweck können Sie die Methode `numpy.digitize()` verwenden.\n",
    "\n",
    "Angenommen, das Grid ist eine Liste von NumPy-Arrays mit den folgenden Aufteilungspunkten:\n",
    "```\n",
    "[array([-0.8, -0.6, -0.4, -0.2,  0.0,  0.2,  0.4,  0.6,  0.8]),\n",
    " array([-4.0, -3.0, -2.0, -1.0,  0.0,  1.0,  2.0,  3.0,  4.0])]\n",
    "```\n",
    "\n",
    "Hier sind einige potenzielle Beispiele und ihre entsprechenden diskretisierten Darstellungen:\n",
    "```\n",
    "[-1.0 , -5.0] => [0, 0]\n",
    "[-0.81, -4.1] => [0, 0]\n",
    "[-0.8 , -4.0] => [1, 1]\n",
    "[-0.5 ,  0.0] => [2, 5]\n",
    "[ 0.2 , -1.9] => [6, 3]\n",
    "[ 0.8 ,  4.0] => [9, 9]\n",
    "[ 0.81,  4.1] => [9, 9]\n",
    "[ 1.0 ,  5.0] => [9, 9]\n",
    "```\n",
    "\n",
    "**Wichtig**: Es kann einmalige Unterschiede beim Binning aufgrund von Fließkomma-Ungenauigkeiten geben, wenn die Samples nahe an den Grid-Grenzen liegen, aber das ist in Ordnung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize(sample, grid):\n",
    "    \"\"\"Discretize a sample as per given grid.\n",
    "    \n",
    "    Args:\n",
    "        sample [array]: Single sample from the (original) continuous space.\n",
    "        grid [list]: List of arrays containing split points for each dimension.\n",
    "    \n",
    "    Returns:\n",
    "        discretized_sample [array]: Sequence of integers with the same number of dimensions as sample.\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    return []\n",
    "\n",
    "# Test implementation\n",
    "grid = create_uniform_grid([-1.0, -5.0], [1.0, 5.0])\n",
    "samples = np.array(\n",
    "    [[-1.0 , -5.0],\n",
    "     [-0.81, -4.1],\n",
    "     [-0.8 , -4.0],\n",
    "     [-0.5 ,  0.0],\n",
    "     [ 0.2 , -1.9],\n",
    "     [ 0.8 ,  4.0],\n",
    "     [ 0.81,  4.1],\n",
    "     [ 1.0 ,  5.0]])\n",
    "discretized_samples = np.array([discretize(sample, grid) for sample in samples])\n",
    "print(\"\\nSamples:\", repr(samples), sep=\"\\n\")\n",
    "print(\"\\nDiscretized samples:\", repr(discretized_samples), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assertions, please do not change\n",
    "assert len(discretize(np.array([[-24.2, -3.1], [0.1011, 0.10009]]), create_uniform_grid([-42., -3.14], [0.1, 0.11], (3, 5)))) == 2, 'Output shape not matching input shape. Please check implementation'\n",
    "assert np.allclose(discretize(np.array([[-24.2, -3.1], [0.1011, 0.10009]]), create_uniform_grid([-42., -3.14], [0.1, 0.11], (3, 5)))[0], np.array([1, 2])), 'Discretized samples are not as expected. Please check implementation'\n",
    "assert np.allclose(discretize(np.array([[-24.2, -3.1], [0.1011, 0.10009]]), create_uniform_grid([-42., -3.14], [0.1, 0.11], (3, 5)))[1], np.array([4, 4])), 'Discretized samples are not as expected. Please check implementation'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diskretisierte Features: 'Cart Position' und 'Cart Velocity'\n",
    "\n",
    "Nachfolgend werden, wie eingangs exemplarisch gezeigt, die diskretisierten Features des `CartPole` Environment anhand von zufällig gesampleten Stichproben visualisiert.\n",
    "\n",
    "**Wichtig**: Bei den Ober- und Untergrenzen für das Grid ist Vorsicht geboten, wenn man direkt die Environment Grenzen einsetzt (`env.observation_space.low` und `env.observation_space.high`), da diese zu `inf` bzw `-inf` Werten im Grid führen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low = [-4.800e+00, -3.403e+38, -4.189e-01, -3.403e+38]\n",
    "high = [4.800e+00, 3.403e+38, 4.189e-01, 3.403e+38]\n",
    "\n",
    "state_grid = create_uniform_grid(low[:2], high[:2], bins=(10, 10))\n",
    "print('State grid: ', state_grid)\n",
    "\n",
    "# Get samples from the state space, discretize and visualize them\n",
    "state_samples = np.array([env.observation_space.sample()[:2] for i in range(10)])\n",
    "discretized_state_samples = np.array([discretize(sample, state_grid) for sample in state_samples])\n",
    "\n",
    "visualize_samples(state_samples, discretized_state_samples, state_grid, low[:2], high[:2],\n",
    "                  x_label='Cart position', y_label='Cart velocity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diskretisierte Features: 'Pole Angle' und 'Pole Velocity At Tip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_grid = create_uniform_grid(low[2:], high[2:], bins=(10, 10))\n",
    "print('State grid: ', state_grid)\n",
    "\n",
    "# Get samples from the state space, discretize and visualize them\n",
    "state_samples = np.array([env.observation_space.sample()[2:] for i in range(10)])\n",
    "discretized_state_samples = np.array([discretize(sample, state_grid) for sample in state_samples])\n",
    "\n",
    "visualize_samples(state_samples, discretized_state_samples, state_grid, low[2:], high[2:],\n",
    "                 x_label='Pole angle', y_label='Pole velocity at tip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 2.2: Q-Learning mit CartPole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit Hilfe des vereinfachten State-Spaces soll nun, wie bei dem `CliffWalking` Environment, das `CartPole` Environment mittels Q-Learning gelöst werden. Ähnlich wie zuvor müssen hier dazu wieder die Methoden `act` und `train` implementiert werden. Dazu kommt, dass der Agent dieses Mal vom `AbstractAgent` erbt und die Klasse entsprechend initialisiert werden muss (siehe `# TODO` im Code). Zusätzlich sollten die Hyperparameter (`epsilon` und `alpha`) aktualisiert werden (Stichwort 'epsilon-/alpha-decay'), nachdem eine Episode abgeschlossen wurde (`done` ist `True`). Der Main-Loop ist dieses Mal gegeben. Dort wird die oben implementierte Diskretisierung angewendet:\n",
    "\n",
    "- `__init__`: Konstruktor der agent Klassen\n",
    "- `act`: Implementiert $\\epsilon$-greedy Exploration-Strategy\n",
    "- `train`: Lernen der Q-Funktion anhand der gesammelten Erfahrungen\n",
    "\n",
    "**Wichtig**: Um das entsprechende `PASSED` für das CartPole Environment zu bekommen, muss, wie oben beschrieben, ein durchschnittlicher Reward >= 195,0 über 100 aufeinanderfolgende Episoden erreicht werden. Es ist empfehlenswert, nach der Implementierung des Agenten, sich die Hyperparameter genauer anzuschauen, um die optimalen Parameter für die Initialisierung des Agenten zu finden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent(AbstractAgent):\n",
    "\n",
    "    def __init__(self, state_grid, action_size, gamma=None, epsilon=None, epsilon_min=None,\n",
    "                 alpha=None, alpha_min=None):\n",
    "        self.state_grid = state_grid\n",
    "        self.state_size = tuple(len(splits) + 1 for splits in self.state_grid)\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        print(\"State space size:\", self.state_size)\n",
    "        print(\"Action space size:\", self.action_size)\n",
    "\n",
    "        # Hyperparameter\n",
    "        self.gamma = gamma # discount factor (how much discount future reward)\n",
    "        self.epsilon = epsilon # exploration rate for the agent\n",
    "        self.alpha = alpha # learning rate\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.alpha_min = alpha_min\n",
    "\n",
    "        # Initialize Q[s,a] table\n",
    "        self.Q = np.array([]) # TODO\n",
    "        print(\"Q table size:\", self.Q.shape)\n",
    "        \n",
    "        self.t = 0 # played episodes\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"Selects the action to be executed based on the given state.\n",
    "        \n",
    "        Implements epsilon greedy exploration strategy, i.e. with a probability of\n",
    "        epsilon, a random action is selected.\n",
    "        \n",
    "        Args:\n",
    "            state [tuple]: Tuple of agent and target position, representing the state.\n",
    "        \n",
    "        Returns:\n",
    "            action [int]\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        return 0\n",
    "\n",
    "    def train(self, experience):\n",
    "        \"\"\"Learns the Q-values based on experience.\n",
    "        \n",
    "        Args:\n",
    "            experience [tuple]: Tuple of state, action, next state, reward, done.\n",
    "        \n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "\n",
    "        if done:\n",
    "            # Update hyperparameter\n",
    "            # TODO\n",
    "            self.t += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assertions, please do not change\n",
    "test_agent = QLearningAgent([np.array([2,3]),np.array([1,2,3])], 3, gamma=5, epsilon=0.8, alpha=2)\n",
    "assert test_agent.Q.shape == (3, 4, 3), 'Q-table in agent should be initialized correctly. Please check init method'\n",
    "np.random.seed(42)\n",
    "test_agent.action_size = 3; test_agent.epsilon = 1.0; test_agent.Q = np.array([[[[[.5, 0.],[0., 0.]],[[0., 0.],[0., 0.]]],[[[0., 0.],[0., 0.]],[[0., 0.],[0.,0.]]]],[[[[0., 0.],[0., 0.]],[[0., 0.],[0., 0.]]],[[[0., 0.],[0., 0.]],[[0., 0.],[.1, .2]]]]])\n",
    "assert test_agent.act((1,2,3,1)) == 0, 'Agent should explore the environment. Please check agent act method'\n",
    "test_agent.epsilon = 0.0\n",
    "assert test_agent.act((1,1,1,1)) == 1, 'Agent should exploit the environment. Please check agent act method'\n",
    "test_agent.train(((1,1,1,1),1,(0,0,0,0),5,False))\n",
    "assert np.allclose(test_agent.Q, np.array([[[[[.5, 0.],[0., 0.]],[[0., 0.],[0., 0.]]],[[[0., 0.],[0., 0.]],[[0., 0.],[0.,0.]]]],[[[[0., 0.],[0., 0.]],[[0., 0.],[0., 0.]]],[[[0., 0.],[0., 0.]],[[0., 0.],[.1, 14.8]]]]])), 'Q update incorrect. Please check agent train method'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import suppress\n",
    "\n",
    "def interact_with_environment(env, agent, n_episodes=1000, max_steps=200, train=True, verbose=False):      \n",
    "    statistics = []\n",
    "    \n",
    "    with suppress(KeyboardInterrupt):\n",
    "        for episode in range(n_episodes):\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            state = env.reset()\n",
    "            state = tuple(discretize(state, agent.state_grid)) # transform state\n",
    "\n",
    "            if verbose and episode == n_episodes-1:\n",
    "                img = plt.imshow(env.render(mode='rgb_array'))\n",
    "            \n",
    "            for t in range(max_steps):\n",
    "                action = agent.act(state)\n",
    "                \n",
    "                if verbose and episode == n_episodes-1:\n",
    "                    img.set_data(env.render(mode='rgb_array'))\n",
    "                    plt.axis('off')\n",
    "                    display.display(plt.gcf())\n",
    "                    display.clear_output(wait=True)\n",
    "                \n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                next_state = tuple(discretize(next_state, agent.state_grid))\n",
    "\n",
    "                if train:\n",
    "                    agent.train((state, action, next_state, reward, done))\n",
    "\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            if verbose and episode % 10 == 0:\n",
    "                print(\"\\rEpisode: {}/{} | Score: {}\".format(episode, n_episodes, total_reward), end=\"\")\n",
    "                sys.stdout.flush()\n",
    "\n",
    "            statistics.append({\n",
    "                'episode': episode,\n",
    "                'score': total_reward,\n",
    "                'steps': t\n",
    "            })\n",
    "        \n",
    "    return statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "# TODO: Find better values\n",
    "gamma = 0.75\n",
    "epsilon = 0.75\n",
    "epsilon_min = 0.1\n",
    "alpha = 0.75\n",
    "alpha_min = 0.1\n",
    "bins = (10, 10, 10, 10)\n",
    "\n",
    "# State grid\n",
    "low = [-4.800e+00, -3.403e+38, -4.189e-01, -3.403e+38]\n",
    "high = [4.800e+00, 3.403e+38, 4.189e-01, 3.403e+38]\n",
    "state_grid = create_uniform_grid(low, high, bins=bins)\n",
    "\n",
    "agent = QLearningAgent(state_grid=state_grid, action_size=env.action_space.n, gamma=gamma, \n",
    "                       epsilon=epsilon, epsilon_min=epsilon_min, alpha=alpha, alpha_min=alpha_min)\n",
    "\n",
    "statistics = interact_with_environment(env, agent, n_episodes=1000, verbose=True)\n",
    "plot_statistics(statistics)\n",
    "\n",
    "assert len(statistics) >= 100, 'Please run at least 100 episodes for validating result'\n",
    "print('Mean over last 100 episodes: {}'.format(np.mean(list(map(itemgetter('score'), statistics[-100:])))))\n",
    "check_test.run_check('mean_score_check', statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz: Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frage 1: Wie groß ist der CartPole Zustandsraum?\n",
    "\n",
    "Laut Schätzungen von Astonomen besteht das Universum aus etwa $10^{80}$ Atome. Ausgeschrieben sind das  100000000000000000000000000000000000000000000000000000000000000000000000000000000 Atome. \n",
    "\n",
    "Wie viele Zustände können in dem CartPole Environment auftreten, wenn die nachfolgend angegebenen Wertebereiche betrachtet werden?\n",
    "\n",
    "- Cart Position ∈ [-4.8, 4.8]\n",
    "- Cart Velocity ∈ [-3.4 10^38, 3.4 10^38]\n",
    "- Pole Angle ∈ [-0.42, 0.42]\n",
    "- Pole Velocity At Tip ∈ [-3.4 10^38, 3.4 10^38]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antwort: # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frage 2: Was könnte man bei der Diskretisierung optimieren?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antwort: # TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
