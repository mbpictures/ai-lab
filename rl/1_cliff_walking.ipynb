{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HSKA AI-Lab RL: Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mount Google Drive as folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "%cd /content/drive/My\\ Drive/ai-lab/rl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal-Difference Methoden\n",
    "\n",
    "In diesem Notebook geht es darum Q-Learning anhand des Beispiels [CliffWalking](https://github.com/openai/gym/blob/master/gym/envs/toy_text/cliffwalking.py) zu implementieren. CliffWalking steht im OpenAI Gym zur Verfügung und wird in der Implementierung (`CliffWalkingEnv`) wie folgt beschrieben:\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "This is a simple implementation of the Gridworld Cliff\n",
    "reinforcement learning task.\n",
    "Adapted from Example 6.6 (page 106) from Reinforcement Learning: An Introduction\n",
    "by Sutton and Barto:\n",
    "http://incompleteideas.net/book/bookdraft2018jan1.pdf\n",
    "With inspiration from:\n",
    "https://github.com/dennybritz/reinforcement-learning/blob/master/lib/envs/cliff_walking.py\n",
    "The board is a 4x12 matrix, with (using Numpy matrix indexing):\n",
    "    [3, 0] as the start at bottom-left\n",
    "    [3, 11] as the goal at bottom-right\n",
    "    [3, 1..10] as the cliff at bottom-center\n",
    "Each time step incurs -1 reward, and stepping into the cliff incurs -100 reward\n",
    "and a reset to the start. An episode terminates when the agent reaches the goal.\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CliffWalking Environment kennenlernen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install gym[atari]==0.12.5\n",
    "\n",
    "import sys\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from IPython.display import Image\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from plot_utils import plot_values\n",
    "import check_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CliffWalking-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Agent bewegt sich durch eine GridWorld mit den Maßen $4\\times 12$, wobei die States wie folgt nummeriert sind:\n",
    "```\n",
    "[[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],\n",
    " [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23],\n",
    " [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35],\n",
    " [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]]\n",
    "```\n",
    "Zu Beginn jeder Episode startet der Agent in dem initialen State `36`. Der State `47` ist der einzige Terminal State. Die \"Klippen\" gehören zu den States `37` bis `46`. Sowohl State- als auch Action-Space sind diskret.\n",
    "\n",
    "Der Agent hat 4 mögliche Actions zur Auswahl:\n",
    "```\n",
    "UP = 0\n",
    "RIGHT = 1\n",
    "DOWN = 2\n",
    "LEFT = 3\n",
    "```\n",
    "\n",
    "Mit $\\mathcal{S}^+=\\{0, 1, \\ldots, 47\\}$ und $\\mathcal{A} =\\{0, 1, 2, 3\\}$ (siehe `action_space` und `observation_space` in der nachfolgenden Zelle). Führt der Agent eine Action aus, welche nicht durchführbar ist (z.B. Action `LEFT` in State `36`), bleibt der Agent in dem vorherigen State und erhält den entsprechenden Reward.\n",
    "\n",
    "Die Reward Funktion ist unten abgebildet. Für jeden \"Schritt\" den der Agent macht, außer dem Wechsel in den Terminal State, wird dieser mit einem negativen Reward von `-1` bestraft. Kommt der Agent in einen `Cliff`-State, wird er mit einem Reward von `-100` bestraft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.action_space)\n",
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward function\n",
    "gridworld = np.full((4,12), -1)\n",
    "gridworld[-1][1:-1] = -100\n",
    "gridworld[-1][-1] = 0\n",
    "\n",
    "plot_values(gridworld, title='Reward Function')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal state-value function\n",
    "V_opt = np.zeros((4,12))\n",
    "V_opt[0:13][0] = -np.arange(3, 15)[::-1]\n",
    "V_opt[0:13][1] = -np.arange(3, 15)[::-1] + 1\n",
    "V_opt[0:13][2] = -np.arange(3, 15)[::-1] + 2\n",
    "V_opt[3][0] = -13\n",
    "\n",
    "plot_values(V_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 1: TD Control: Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In dieser Aufgabe soll Q-Learning implementiert werden, um im CliffWalking Environment die Q-Funktion lernen zu können. Dazu müssen folgende drei Methoden implementiert werden:\n",
    "\n",
    "- `act`: Implementiert $\\epsilon$-greedy Exploration-Strategy\n",
    "- `train`: Lernen der Q-Funktion anhand der gesammelten Erfahrungen\n",
    "- `q_learning`: Main-Loop des Q-Learning Algorithmus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def act(Q_state, action_size, eps):\n",
    "    \"\"\"Selects the action to be executed based on the given state.\n",
    "\n",
    "    Implements epsilon greedy exploration strategy, i.e. with a probability of\n",
    "    epsilon, a random action is selected.\n",
    "\n",
    "    Args:\n",
    "        state [int]: Number of current agent state.\n",
    "        action_size [int]: Size of action space\n",
    "        eps [float]: Hyperparameter epsilon for epsilon-greedy strategy.\n",
    "\n",
    "    Returns:\n",
    "        action [int]\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assertions, please do not change\n",
    "np.random.seed(42)\n",
    "assert act(np.array([1, 2, 3, 4]), 10, 1) == 7, 'Agent should explore the environment. Please check act method'\n",
    "assert act(np.array([1, 2, 3, 4]), 10, 0) == 3, 'Agent should exploit the knowledge about the environment. Please check act method'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(curr_Q_value, next_Q_value, reward, alpha, gamma):\n",
    "    \"\"\"Learns the Q-values based on experience.\n",
    "\n",
    "    Args:\n",
    "        curr_Q_value [float]: Q-value of current state.\n",
    "        next_Q_value [float]: Q-value of next state.\n",
    "        reward [int]: Reward received from environment.\n",
    "        alpha [float]: Hyperparameter alpha as step-size.\n",
    "        gamma [float]: Hyperparameter gamma as discount factor.\n",
    "\n",
    "    Returns:\n",
    "        updated Q value [float]\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assertions, please do not change\n",
    "assert train(-2.44, -1.23, 2.5, 0.77, 0.45) == 0.937605, 'Agent should learn the correct Q-value using Q-learning algorithm. Please check train method'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, num_episodes, alpha, gamma=1.0):\n",
    "    \"\"\"Main-loop of Q-learning algorithm.\n",
    "    \n",
    "    Args:\n",
    "        env [gym.Env]: OpenAI Gym environment.\n",
    "        num_episodes [int]: Number of episodes to train in environment.\n",
    "        alpha [float]: Hyperparameter alpha as step-size.\n",
    "        \n",
    "    Returns:\n",
    "        Q [dict]\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the estimated optimal policy and corresponding action-value function\n",
    "Q_learning = q_learning(env, num_episodes=5000, alpha=0.01)\n",
    "\n",
    "# Print the estimated optimal policy\n",
    "policy_Q_learning = np.array([np.argmax(Q_learning[key]) if key in Q_learning else -1 for key in np.arange(48)]).reshape(4,12)\n",
    "check_test.run_check('td_control_check', policy_Q_learning)\n",
    "print(\"\\nEstimated Optimal Policy (UP = 0, RIGHT = 1, DOWN = 2, LEFT = 3, N/A = -1):\")\n",
    "print(policy_Q_learning)\n",
    "\n",
    "# Plot the estimated optimal state-value function\n",
    "V_Q_learning = ([np.max(Q_learning[key]) if key in Q_learning else 0 for key in np.arange(48)])\n",
    "plot_values(V_Q_learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz: Temporal-Difference Methoden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frage 1: Wie lauten die Werte der Q-Tabelle in folgendem Szenario?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('./assets/session_1_task_1_0.png', width=\"600\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit $\\alpha$ = 0.2 und $gamma$ = 1.0. Der Agent befindet sich in State $S_1$ zum Zeitpunkt $0$ (initialer State).\n",
    "\n",
    "##### Episode 0, Time 0\n",
    "\n",
    "Im initialen State $S_1$ entscheidet sich der Agent für Action \"stay\". Der entsprechende nachfolgende State ist weiterhin $S_1$. Vom Environment erhält der Agent einen Reward von $-1$. Wie sieht die Q-Tabelle aus bei Episode 0, Time 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antwort (runde auf eine Stelle nach dem Komma): # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Episode n, TIme 0\n",
    "\n",
    "Eine neue Episode beginnt und der Agent befindet sich wieder in State $S_1$ und entscheidet sich für die Action \"stay\". Was ist der neue Q-Wert für $Q(S_1, stay)$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('./assets/session_1_task_1_1.png', width=\"600\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antwort (runde auf eine Stelle nach dem Komma): # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frage 2: Was versteht man unter einer Optimal Policy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antwort: # TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
