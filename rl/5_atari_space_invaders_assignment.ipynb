{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLSrt5S0bTxr"
      },
      "source": [
        "# HSKA AI-Lab RL: Assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQZPlubPbTxv"
      },
      "source": [
        "## Mount Google Drive as folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sGYjVgrbTxx"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "%cd /content/drive/My\\ Drive/ai-lab/rl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpMm-fi9bTx-"
      },
      "source": [
        "Im abschließenden Assignment soll nun [Atari Space Invaders](https://github.com/openai/gym/blob/4ede9280f9c477f1ca09929d10cdc1e1ba1129f1/gym/envs/atari/atari_env.py) implementiert werden. Space Invaders zählt zu den klassischen Atari 2600 Spielen und wird hier nahezu unverändert genutzt. Für Details siehe das [Paper von DeepMind](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTy9AcOWbTyC"
      },
      "source": [
        "### Atari Pong Environment vorbereiten"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9umI9SgLbTyD"
      },
      "outputs": [],
      "source": [
        "%tensorflow_version 1.x\n",
        "%pip install --upgrade pip\n",
        "%pip install gym[atari]==0.12.5\n",
        "%pip install pyglet==1.3.2\n",
        "\n",
        "import gym\n",
        "\n",
        "import random\n",
        "from collections import deque\n",
        "from typing import Tuple\n",
        "import time\n",
        "from datetime import datetime\n",
        "from contextlib import suppress\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "from tensorflow.keras.layers import Conv2D, Flatten, Dense, Lambda, multiply, Input\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.losses import huber_loss\n",
        "from tensorflow.keras.backend import set_session\n",
        "from loggers import TensorBoardLogger, tf_summary_image\n",
        "\n",
        "%pip install matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from plot_utils import plot_statistics\n",
        "from abstract_agent import AbstractAgent\n",
        "from atari_helpers import LazyFrames, wrap_deepmind, make_atari\n",
        "\n",
        "!apt-get install -y xvfb python-opengl\n",
        "!python -m pip install pyvirtualdisplay\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "is_ipython = 'inline' in plt.get_backend()\n",
        "if is_ipython:\n",
        "    from IPython import display\n",
        "    from IPython.display import SVG\n",
        "\n",
        "plt.ion()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kv1rVbIobTyL"
      },
      "outputs": [],
      "source": [
        "env = make_atari('SpaceInvadersNoFrameskip-v4', skip=3)\n",
        "env = wrap_deepmind(env, frame_stack=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79E2RKMGbTyQ"
      },
      "source": [
        "### Assignment: Atari Space Invaders\n",
        "\n",
        "Für das Assignment soll der `Agent` mit den Methoden `act`, `train`, `_build_model` und `_replay` implementiert werden. Das Model ist hier nicht mehr vorgegeben und kann entweder aus dem Notebook `4_atari_pong_dqn` übernommen werden oder selbst implementiert werden."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6AUMYRSbTyS"
      },
      "outputs": [],
      "source": [
        "class Agent(AbstractAgent):\n",
        "\n",
        "    def __init__(self, action_size: int, state_size: int,\n",
        "                 gamma: float, epsilon: float, epsilon_decay: float, epsilon_min: float, \n",
        "                 alpha: float):\n",
        "        self.action_size = action_size\n",
        "        self.state_size = state_size\n",
        "\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def _build_model(self):\n",
        "        \"\"\"Model used to learn e.g. action-values.\n",
        "        \n",
        "        Returns:\n",
        "            model [Model]\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def _replay(self) -> None:\n",
        "        \"\"\"Gets random experiences from memory for batch update.\n",
        "        \n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "        \n",
        "    def act(self, state: LazyFrames) -> int:\n",
        "        \"\"\"Selects the action to be executed based on the given state.\n",
        "\n",
        "        Implements epsilon greedy exploration strategy, i.e. with a probability of\n",
        "        epsilon, a random action is selected.\n",
        "\n",
        "        Args:\n",
        "            state [LazyFrames]: LazyFrames object representing the state based on 4 stacked observations (images)\n",
        "\n",
        "        Returns:\n",
        "            action [int]\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def train(self, experience: Tuple[LazyFrames, int, LazyFrames, float, bool]) -> None:\n",
        "        \"\"\"Stores the experience in memory. If memory is full trains network by replay.\n",
        "\n",
        "        Args:\n",
        "            experience [tuple]: Tuple of state, action, next state, reward, done.\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSuv9PstbTyY"
      },
      "outputs": [],
      "source": [
        "def interact_with_environment(env, agent, n_episodes=600, max_steps=1000000, train=True, verbose=True):      \n",
        "    statistics = []\n",
        "    tb_logger = TensorBoardLogger(f'./logs/run-{datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")}')\n",
        "    \n",
        "    with suppress(KeyboardInterrupt):\n",
        "        total_step = 0\n",
        "        for episode in range(n_episodes):\n",
        "            done = False\n",
        "            episode_reward = 0\n",
        "            state = env.reset()\n",
        "            episode_start_time = time.time()\n",
        "            episode_step = 0\n",
        "\n",
        "            while not done:\n",
        "                action = agent.act(state)\n",
        "                next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "                if train:\n",
        "                    agent.train((state, action, next_state, reward, done))\n",
        "\n",
        "                if episode == 0:\n",
        "                    # for debug purpose log every state of first episode\n",
        "                    for obs in state:\n",
        "                        tb_logger.log_image(f'state_t{episode_step}:', tf_summary_image(np.array(obs, copy=False)),\n",
        "                                            global_step=total_step)\n",
        "                state = next_state\n",
        "                episode_reward += reward\n",
        "                episode_step += 1\n",
        "            \n",
        "            total_step += episode_step\n",
        "\n",
        "            if episode % 10 == 0:\n",
        "                speed = episode_step / (time.time() - episode_start_time)\n",
        "                tb_logger.log_scalar('score', episode_reward, global_step=total_step)\n",
        "                tb_logger.log_scalar('epsilon', agent.epsilon, global_step=total_step)\n",
        "                tb_logger.log_scalar('speed', speed, global_step=total_step)\n",
        "                if verbose:\n",
        "                    print(f'episode: {episode}/{n_episodes}, score: {episode_reward}, steps: {episode_step}, '\n",
        "                          f'total steps: {total_step}, e: {agent.epsilon:.3f}, speed: {speed:.2f} steps/s')\n",
        "\n",
        "            statistics.append({\n",
        "                'episode': episode,\n",
        "                'score': episode_reward,\n",
        "                'steps': episode_step\n",
        "            })\n",
        "                                  \n",
        "            if total_step >= max_steps:\n",
        "                break\n",
        "        \n",
        "    return statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOuF3i3FbTye"
      },
      "outputs": [],
      "source": [
        "action_size = env.action_space.n\n",
        "state_size = env.observation_space.shape[0]\n",
        "\n",
        "# Hyperparams (adjust to your own needs!)\n",
        "annealing_steps = 100000 # not episodes!\n",
        "gamma = 0.99\n",
        "epsilon = 1.0\n",
        "epsilon_min = 0.01\n",
        "epsilon_decay = (epsilon - epsilon_min) / annealing_steps\n",
        "alpha = 0.0001\n",
        "\n",
        "agent = Agent(action_size=action_size, state_size=state_size, gamma=gamma, \n",
        "                 epsilon=epsilon, epsilon_decay=epsilon_decay, epsilon_min=epsilon_min, \n",
        "                 alpha=alpha)\n",
        "\n",
        "statistics = interact_with_environment(env, agent, n_episodes=400, verbose=True)\n",
        "env.close()\n",
        "plot_statistics(statistics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xw_I-9ndbTyj"
      },
      "source": [
        "#### Aufbau Keras Model\n",
        "Der Aufbau des Keras-Modells kann zur Verdeutlichung nochmals geplottet werden."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ys6WVhAEbTyk"
      },
      "outputs": [],
      "source": [
        "tf.keras.utils.plot_model(agent.model, to_file='keras_plot_model.png', show_shapes=True)\n",
        "display.Image('keras_plot_model.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BW57njYbTyo"
      },
      "source": [
        "#### Performanceauswertung (Video)\n",
        "Der folgende Code dient zur Performancebewertung des Agenten. Der (hoffentlich) trainierte Agent wird bei seiner Ausführung gefilmt, trainiert aber nicht weiter. Anschließend wird das Video seiner besten Performance dargestellt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tS9ncN9YbTyp"
      },
      "outputs": [],
      "source": [
        "for i in range(3):\n",
        "    state = env.reset()\n",
        "    img = plt.imshow(env.render(mode='rgb_array'))\n",
        "    for j in range(200):\n",
        "        action = agent.act(state)\n",
        "        img.set_data(env.render(mode='rgb_array')) \n",
        "        plt.axis('off')\n",
        "        display.display(plt.gcf())\n",
        "        display.clear_output(wait=True)\n",
        "        state, reward, done, _ = env.step(action)\n",
        "        if done:\n",
        "            break \n",
        "            \n",
        "env.close()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "5_atari_space_invaders_assignment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
